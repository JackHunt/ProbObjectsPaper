To evaluate our system, we perform quantitative experiments on camera pose estimation accuracy, qualitative and quantitative analyses on the obtained reconstructions.
%To evaluate the performance of our system we perform experiments that draw comparisons in terms of quantitative pose estimation and qualitative reconstruction quality and efficacy.
Firstly, the pose estimation accuracy is evaluated via an established SLAM evaluation benchmark~\cite{sturm12iros}.
We highlight that % with the primary difference being that with
in traditional dense SLAM systems~\cite{Prisacariu2014,Niessner2013,Newcombe2011} -- for which the benchmark is often employed -- the entire contents of the visible scene are used for pose estimation, whereas in our system we rely only on points belonging to the object's surface.
Whilst more challenging, this implicitly allows us to track the camera w.r.t. the object regardless of which of the two is subject to motion.
Then, qualitative comparisons are drawn between the reconstructions attained by our system, and those of the method described in \cite{Ren2013}.
We evaluate our system on multiple frame sequences depicting objects of different sizes. %; some with the object moving wrt. a fixed camera, others with the sensor in motion.% in the case of qualitative evaluation.
Finally, both qualitative and quantitative comparisons are drawn to an implementation \cite{Prisacariu2014} of the KinectFusion pipeline \cite{Newcombe2011}.

%\vspace{-.7\baselineskip}

\subsection{Pose Estimation Quality}
In this section we present quantitative results of our systems' %ability to maintain tracking
robustness in estimating the camera motion, by performing tracking against the reconstruction of a single object, instead of the whole scene.
The trajectories estimated by our system demonstrate low tracking drift. % and a robustness to loop closure events.
We perform such evaluation on two sequences of the RGB-D SLAM Dataset~\cite{sturm12iros} depicting static objects observed by a moving camera.
Tracking is performed using purely geometric cues, by matching the current depth frame with a rendering of the reconstructed object using ICP. We compare results against the system of \cite{Prisacariu2014} 
with object segmentation such that the comparison system also only tracks the object.
%, as such the evaluation is of camera pose estimation when tracking an object.
%By tracking the sensor pose against a subset of the observed scene 
%At this point it should be highlighted that our system is at a disadvantage when compared to dense SLAM systems that utilise the entire scene geometry for pose optimisation.

%In the following experiments, tracking is performed using only geometry cues from the rendered object models and the instantaneous depth frame.\\

The tracking accuracy is evaluated via the ATE (Absolute Trajectory Error) metric \cite{sturm12iros}, and is summarised in Table~\ref{ateTable}.

\begin{table}[!t]
	{
        \footnotesize
		\begin{center}
			\begin{tabular}{l@{\hskip 1cm} c c}
				\emph{Sequence Name} & \emph{Our Approach ATE (m)} & \emph{InfiniTAM ATE (m)}\\
				\midrule
				\textsf{freiburg3\_cabinet} & 0.077903 & 0.520693\\
				\textsf{freiburg3\_teddy}   & 0.030596 & 0.048560 
			\end{tabular}
		\end{center}
	}
   \vspace{-4mm}
	\caption{ATE (Absolute Trajectory Error) results (lower is better) achieved by our approach versus InfiniTAM with object segmentation.}
	\label{ateTable}
\end{table}

\begin{figure}[!t]
	\centering
	\begin{tabular}{cc}
		%\fbox{\includegraphics[width=0.2\textwidth]{results/rgbd_dataset_freiburg3_cabinet.png}} & %\fbox{\includegraphics[width=0.2\textwidth]{results/rgbd_dataset_freiburg3_teddy.png}}
		\includegraphics[width=0.2\textwidth]{results/comparative_cabinet.png} &
		\includegraphics[width=0.2\textwidth]{results/comparative_teddy_clean.png}
	\end{tabular}
    \vspace{-3mm}
	\caption{
        Comparison of the estimated camera trajectory with the ground truth for
		~\textit{freiburg3\_cabinet} (left)  and ~\textit{freiburg3\_teddy} (right).
	}
\label{fig:tumTrajectories}
\end{figure}

At this point, it should be highlighted that our proposed system is at a disadvantage when compared to dense SLAM systems that utilise the entire scene geometry for pose optimisation, since we track the sensor/object pose against a subset of the observed scene.
Nevertheless, as shown by the results in Figure~\ref{fig:tumTrajectories}, our system is able to robustly estimate trajectories close to the ground truth whilst using only the objects' geometric appearance.
%Quantitatively, the tracking accuracy is evaluated via the Absolute Trajectory Error (ATE) metric, as outlined by \cite{sturm12iros}.
The cabinet reconstructed in the \textit{freiburg3\_cabinet} sequence is lacking in geometric features, as the object is mostly planar,
%It can be seen that there is a
and the small deficit in tracking quality is mostly due to this factor.
However, our system remains able to estimate a fairly accurate trajectory. 
In the \textit{freiburg3\_teddy} sequence we determine a trajectory very close to the ground truth.
Improvement over the accuracy in \textit{freiburg3\_cabinet} is due to the wider availability of geometrical features, such as curves in the teddy's body and head.

\subsection{Qualitative Reconstruction Quality}
In this section we present a qualitative comparison of our method against the approach by Ren et~al.~\cite{Ren2013} in the reconstruction of closed object models. %by relying on a variety of sequences and demonstrating efficacy over \cite{Ren2013} in this regard.
Each sequence is run through both systems; to evaluate the obtained results we regularly take snapshots of the reconstruction in the case of our system, and the level set evolutions, in the case of Ren et al.
Such snapshots are captured after each quarter of a sequence has been processed. %quarterly intervals of the systems run time through the sequence.

\begin{figure}[!t]
	\centering
	%\begin{tabular}{c}
		\includegraphics[width=0.45\textwidth]{filmstrips/dino.png} \\
		\vspace{0.5mm}
		\includegraphics[width=0.45\textwidth]{filmstrips/dino_s3d_large.png}
	%\end{tabular}
	\caption{
        Quarterly interval snapshots of the Dinosaur Head reconstruction using our method (upper), and the one proposed by Ren et~al.~\cite{Ren2013} (lower).
	}
	\label{fig:dinoComparison}
\end{figure}

\begin{figure}[!t]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=3cm]{screenshots/untextured/teddy_top00.png}&
		\includegraphics[width=3cm]{screenshots/untextured/dino_top00.png}\\
		\includegraphics[width=3cm]{screenshots/untextured/chair_top00.png}&
		\includegraphics[width=3cm]{screenshots/untextured/rock_top00.png}\\
	\end{tabular}
    \vspace{-3mm}
	\caption{
		Closed reconstructions of a Teddy, a Dinosaur Head, a Chair and a Rock.
	}
	\label{fig:top_shots}
%	\vspace{-\baselineskip}
\end{figure}

\begin{figure}[!t]
	\centering
%	\begin{tabular}{cc}
		\includegraphics[width=0.45\textwidth]{filmstrips/rock.png} \\
		\vspace{0.5mm}
	    \includegraphics[width=0.45\textwidth]{filmstrips/rock_s3d_large.png}
%	\end{tabular}
	\caption{
		Quarterly interval snapshots of the Museum Rock reconstruction using our method (top row), and the one proposed by Ren et~al.~\cite{Ren2013} (bottom row).
	}
	\label{fig:rockComparison}
\end{figure}

As depicted in Figure~\ref{fig:dinoComparison}, our method is able to successfully reconstruct the Dinosaur Head, whereas the approach by Ren~et~al. fails to converge towards a feasible shape.
In addition, Figure~\ref{fig:top_shots} demonstrates that our system is able to generate consistent, closed models (unaffected by camera tracking drift) for a variety of sequences containing several loop closures.
Failure of the competing method %of Ren et al
is also apparent for other sequences evaluated in this work, all presenting failure cases analogous to Figure~\ref{fig:dinoComparison}. % to converge to a correct shape.
%Such examples will be presented in the supplementary materials.
%The supplementary materials to this work demonstrate our efficacy vs that of Ren et al \cite{Ren2013}.\\
Another such example may be observed in Figure \ref{fig:rockComparison}.

The object reconstructions depicted in Figure~\ref{fig:demo} have been obtained from sequences in which a camera was moved in a loop around each object in order to generate a closed model.

\begin{figure*}[!t]
	\centering
	\begin{tabular}{cccc}
		\includegraphics[width=3cm]{screenshots/Comparison Stills/InfiniTAM/bear00.png}&
		\includegraphics[width=3cm]{screenshots/Comparison Stills/InfiniTAM/brain00.png}&
		\includegraphics[width=3cm]{screenshots/Comparison Stills/InfiniTAM/chair00.png}&
		\includegraphics[width=3cm]{screenshots/Comparison Stills/InfiniTAM/dino00.png}\\
		\includegraphics[width=3cm]{screenshots/Comparison Stills/ProbObjects/bear00.png}&
		\includegraphics[width=3cm]{screenshots/Comparison Stills/ProbObjects/brain00.png}&
		\includegraphics[width=3cm]{screenshots/Comparison Stills/ProbObjects/chair00.png}&
		\includegraphics[width=3cm]{screenshots/Comparison Stills/ProbObjects/dino00.png}\\
		\includegraphics[width=3cm]{hausdorff/bear.png}&
		\includegraphics[width=3cm]{hausdorff/brain.png}&
		\includegraphics[width=3cm]{hausdorff/chair.png}&
		\includegraphics[width=3cm]{hausdorff/dino.png}\\
	\end{tabular}
	\vspace{-3mm}
	\caption{
		From left to right, Bear, Brain, Chair and Dinosaur Head. Row 1 is InfiniTAM output, row 2 is our system and row 3 is the Hausdorff distance between the two, with the colour scale 
		given in Figure \ref{fig:colour_range}.}
	\vspace{-5mm}
	\label{fig:demo}
\end{figure*}

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{hausdorff/colour_range.png}
	\caption{Hausdorff distance heatmap key. Blue is \textit{Max Dist} and red is \textit{Min Dist} of Table \ref{hausdorffTable}.}
	\label{fig:colour_range}
\end{figure}

In addition, we provide a qualitative comparison between the reconstructions produced by the KinectFusion implementation of \cite{Prisacariu2014} when 
tracking the entire scene and manually segmenting the object out as a post processing step against our online segmentation and reconstruction system. This 
comparison is visible in Figure \ref{fig:demo}

%As can be seen from the top down views of Figure \ref{fig:top_shots}, our system is capable of producing closed models, unaffected by tracking drift and loop closure events.
Finally, we provide an example of a typical failure case when using the standard KinectFusion pipeline vs our approach. The gaps on the object surface in Figure \ref{fig:gappy_teddy} 
is caused by tracking drift, resulting in the system fusing surface information in to the wrong region of space, as can be seen, this is remedied by our approach.
\begin{figure}[!t]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=3cm]{screenshots/GappyTeddy/one_scene00.png}&
		\includegraphics[width=3cm]{screenshots/GappyTeddy/one_scene01.png}\\
		\includegraphics[width=3cm]{screenshots/GappyTeddy/multi_scene00.png}&
		\includegraphics[width=3cm]{screenshots/GappyTeddy/multi_scene01.png}\\
	\end{tabular}
	\vspace{-3mm}
	\caption{
		Teddy reconstruction with InfiniTAM (first row) versus our system (second row).
	}
	\label{fig:gappy_teddy}
\end{figure}

\subsection{Quantitative Reconstruction Quality}
In this section we perform a quantitative evaluation of reconstruction quality of our method against an established dense SLAM system \cite{Prisacariu2014} following the KinectFusion \cite{Newcombe2011} pipeline.
The outputted reconstructions of our model are compared with the reconstruction of the dense SLAM system with the object of interest manually segmented out from the remainder of the scene. To quantify the reconstruction quality 
we employ the Hausdorff Distance \cite{Hausdorff} for subsets of metric spaces, where in our case the metric space is Euclidean. The Hausdorff Distance is defined as follows
\begin{equation}
\begin{split}
d_{H}(X, Y) = \max \Bigg \{ \sup_{x \in X} \inf_{y \in Y} d(x, y), \sup_{y \in Y} \inf_{x \in X} d(x, y) \Bigg \}
\end{split}
\end{equation}
where $X$ is the ground truth dense SLAM reconstruction, $Y$ is the reconstruction outputted by our system and $d(.)$ is the Euclidean distance.

The resultant comparisons may be found in Table \ref{hausdorffTable}. In addition, we provide the outputted reconstructions from our system textured on reconstruction quality w.r.t. the Hausdorff Distance. 
For reference, the colour scale used is given in figure \ref{fig:colour_range} where the left extrema (blue) is given by a sequences \textit{Max Dist} and the right extrema (red) by the sequences \textit{Min Dist}. 
\begin{table}[!t]
	{
		\footnotesize
		\begin{center}
			\begin{tabular}{l c c c c}
				\emph{Sequence} & \emph{Min Dist} & \emph{Max Dist} & \emph{Mean Dist} & \emph{RMS}\\
				\midrule
				\textsf{Bear} & 0 & 0.102777 & 0.013588 & 0.019796 \\
				\textsf{Brain} & 0 & 0.026465 & 0.008745 & 0.011349 \\
				\textsf{Chair} & 0 & 0.053441 & 0.012349 & 0.016422 \\
				\textsf{Dinosaur Head} & 0 & 0.035252 & 0.007919 & 0.010676
			\end{tabular}
		\end{center}
	}
\vspace{-3mm}
	\caption{Hausdorff Distance measurements between ground truth mesh and our systems output.}
	\label{hausdorffTable}
\end{table}
As can be seen by the similarity measures presented, our system is capable of yielding reconstructions to a high quality despite the much more difficult tracking scenario of a single object rather than an entire 
scene. It can be seen that our output reconstructions are geometrically close to those generated with a dense SLAM system \cite{Prisacariu2014} following the KinectFusion \cite{Newcombe2011} pipeline that 
is modelling and tracking the entire scene.

\subsection{Running Times and Performance}
We have implemented our system both on the CPU and GPU. With a GPU implementation with NVIDIA CUDA we are able to achieve runtimes of on average \textit{90Hz} with a consumer grade NVIDIA GeForce GTX1060 with 
6GB GRAM. With our CPU only implementation we make use of parallelism with OpenMP and achieve runtimes of on average \textit{5Hz} on a consumer grade PC with an Intel Core i5-6600K 3.5GHz CPU and 16GB of RAM. Such 
online runtimes are possible due to the asynchronous way in which we have implemented the online adjustments combined with GPU acceleration.