\noindent The availability of high-quality, consumer-grade depth sensing equipment and the ever-increasing power of modern GPUs have led to major advances in 3D scene reconstruction, with a number of approaches now able to yield dense, globally-consistent models at scale. However, much less progress has been made for objects, which can exhibit far fewer unambiguous geometric/texture cues than a full scene, and thus be much harder to track against. Recently, robust, globally-consistent scene reconstruction has been achieved by combining a multi-segment representation with loop closure detection and an online model correction algorithm. The multi-segment approach naturally reduces drift, and the correction algorithm further refines the resulting model. In this paper, we show how to extend this approach to reconstruct accurate, globally-consistent object models. At the heart of our approach is a novel, probabilistic fusion framework that we use to separate the object of interest from the surrounding scene. We perform both qualitative and quantitative experiments to compare our approach to the current state-of-the-art, and demonstrate compelling improvements in both pose estimation and model quality.